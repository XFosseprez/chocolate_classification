{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c4eb364-e103-4a2b-a5af-c7645140e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd #version 2.3.3\n",
    "import numpy as np #version 2.3.1\n",
    "import seaborn as sns #version 0.13.2\n",
    "import matplotlib #version 3.10.0\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm #version 0.14.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a4e1c74e-86aa-4bfe-82b7-9ff119b7b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data load\n"
     ]
    }
   ],
   "source": [
    "data_stream = load_dataset(\n",
    "    \"openfoodfacts/product-database\",\n",
    "    split=\"food\",\n",
    "    streaming=True # Use streaming for massive datasets\n",
    "    )\n",
    "df_initial = pd.DataFrame(data_stream.take(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2a737430-fa14-4546-b8ee-323ca9e15a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_keywords = [\"name\", \"Quantity\", \"Brands\", \"Categories\", \n",
    "                    \"Manufacturing\", \"Stores\", \"Country\", \n",
    "                     \"Ingredients\",\"Origin\", \"nutriments\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "28f5e04f-2a35-49bb-b98b-b893eafaab4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brands_tags',\n",
       " 'brands',\n",
       " 'categories',\n",
       " 'categories_tags',\n",
       " 'categories_properties',\n",
       " 'ciqual_food_name_tags',\n",
       " 'generic_name',\n",
       " 'ingredients_analysis_tags',\n",
       " 'ingredients_from_palm_oil_n',\n",
       " 'ingredients_n',\n",
       " 'ingredients_original_tags',\n",
       " 'ingredients_original_tags',\n",
       " 'ingredients_percent_analysis',\n",
       " 'ingredients_tags',\n",
       " 'ingredients_text',\n",
       " 'ingredients_with_specified_percent_n',\n",
       " 'ingredients_with_unspecified_percent_n',\n",
       " 'ingredients_without_ciqual_codes_n',\n",
       " 'ingredients_without_ciqual_codes',\n",
       " 'ingredients',\n",
       " 'known_ingredients_n',\n",
       " 'manufacturing_places_tags',\n",
       " 'manufacturing_places',\n",
       " 'nutriments',\n",
       " 'origins_tags',\n",
       " 'origins',\n",
       " 'product_name',\n",
       " 'product_quantity_unit',\n",
       " 'product_quantity',\n",
       " 'quantity',\n",
       " 'serving_quantity',\n",
       " 'stores_tags',\n",
       " 'stores',\n",
       " 'unknown_ingredients_n']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_df_by_keywords(df, keywords):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to include only columns whose names contain any of the given keywords.\n",
    "    \"\"\"\n",
    "    # Create a list of columns where the column name (lowercased) contains any of the keywords\n",
    "    relevant_cols = []\n",
    "    for col in df.columns: \n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in col.lower():\n",
    "                relevant_cols.append(col)\n",
    "    return relevant_cols\n",
    "\n",
    "target_col = filter_df_by_keywords(df_initial, targeted_keywords)\n",
    "target_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a0ec8a12-46fd-490d-a6d4-64a38941d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brands_tags', 'brands', 'categories_tags', 'categories_properties', 'ciqual_food_name_tags', 'generic_name', 'ingredients_n', 'ingredients_original_tags', 'ingredients_original_tags', 'ingredients_percent_analysis', 'ingredients_tags', 'ingredients', 'known_ingredients_n', 'manufacturing_places_tags', 'manufacturing_places', 'nutriments', 'origins_tags', 'origins', 'product_name', 'product_quantity_unit', 'product_quantity', 'serving_quantity', 'stores_tags', 'stores', 'unknown_ingredients_n']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 25)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items_to_remove = [\"categories\", \"categories_properties\t\",\"ingredients_analysis_tags\", \n",
    "                   \"ingredients_from_palm_oil_n\", \"quantity\", \"ingredients_text\",\n",
    "                  \"ingredients_with_specified_percent_n\", \"ingredients_with_unspecified_percent_n\",\n",
    "                  \"ingredients_without_ciqual_codes_n\", \"ingredients_without_ciqual_codes\",\n",
    "                 \" known_ingredients_n\"]\n",
    "removal_set = set(items_to_remove)\n",
    "cleaned_list = [\n",
    "        item for item in target_col\n",
    "        if item not in removal_set\n",
    "    ]\n",
    "\n",
    "print(cleaned_list)\n",
    "df_initial[cleaned_list].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3bbbab5-083c-4645-bbb7-7db609702919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_target_products(example):\n",
    "    \"\"\"\n",
    "    Filters records to find products classified as 'chocolate' AND\n",
    "    containing 'cocoa' in the ingredients list, safely handling lists, strings,\n",
    "    and lists of dictionaries for both fields.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process Categories (Must contain 'chocolate') ---\n",
    "    # Try 'categories_tags', fall back to 'categories' if needed\n",
    "    categories_data = example.get('categories_tags', example.get('categories', ''))\n",
    "    \n",
    "    searchable_categories = ''\n",
    "    if isinstance(categories_data, list):\n",
    "        # Check if the list contains dictionaries \n",
    "        if categories_data and isinstance(categories_data[0], dict):\n",
    "            # Extract the 'name' or 'id' from each dictionary item\n",
    "            string_parts = [item.get('name', item.get('id', '')) for item in categories_data if isinstance(item, dict)]\n",
    "            searchable_categories = ' '.join(string_parts).lower()\n",
    "        else:\n",
    "            # Assume it's a list of strings and join them\n",
    "            searchable_categories = ' '.join(categories_data).lower()\n",
    "    else:\n",
    "        # It's a string, NaN, or other single value\n",
    "        searchable_categories = str(categories_data).lower()\n",
    "        \n",
    "    has_chocolate_category = 'chocolate' in searchable_categories\n",
    "    \n",
    "    \n",
    "    # --- 2. Process Ingredients (Must contain 'cocoa') ---\n",
    "    # Prioritize 'ingredients_text' (the clean string), fall back to 'ingredients'\n",
    "    ingredients_data = example.get('ingredients_text', example.get('ingredients', ''))\n",
    "    \n",
    "    searchable_ingredients = ''\n",
    "    if isinstance(ingredients_data, list):\n",
    "        # Check if the list contains dictionaries (the cause of the TypeError)\n",
    "        if ingredients_data and isinstance(ingredients_data[0], dict):\n",
    "            # Critical fix: Extract the 'text' key from the dictionary objects\n",
    "            string_parts = [item.get('text', '') for item in ingredients_data if isinstance(item, dict)]\n",
    "            searchable_ingredients = ' '.join(string_parts).lower()\n",
    "        else:\n",
    "            # Assume it's a list of strings\n",
    "            searchable_ingredients = ' '.join(ingredients_data).lower()\n",
    "    else:\n",
    "        # It's a string (like the pre-joined ingredients_text), NaN, or other single value\n",
    "        searchable_ingredients = str(ingredients_data).lower()\n",
    "    \n",
    "    has_cocoa_ingredient = 'cocoa' in searchable_ingredients\n",
    "    \n",
    "    return has_chocolate_category and has_cocoa_ingredient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6740baa6-de5c-4b19-817d-7dbae78865a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data load and targeted filtering...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      3\u001b[39m data_stream = load_dataset(\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mopenfoodfacts/product-database\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     split=\u001b[33m\"\u001b[39m\u001b[33mfood\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     streaming=\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# Use streaming for massive datasets\u001b[39;00m\n\u001b[32m      7\u001b[39m )\n\u001b[32m      9\u001b[39m filtered_data_stream = data_stream.filter(filter_target_products)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df_initial = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_data_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m interest_column = [\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mQuantity\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mBrands\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mCategories\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     13\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33mManufacturing\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mStores\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     14\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33mCountry\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mNutrition\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mIngredients\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m                   \u001b[33m\"\u001b[39m\u001b[33mOrigin\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     17\u001b[39m df_keywords_filtered = filter_df_by_keywords(df_initial, keywords)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\pandas\\core\\frame.py:847\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    845\u001b[39m         data = np.asarray(data)\n\u001b[32m    846\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m         data = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) > \u001b[32m0\u001b[39m:\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[32m0\u001b[39m]):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:2226\u001b[39m, in \u001b[36mIterableDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2223\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m formatter.format_row(pa_table)\n\u001b[32m   2224\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2226\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_typed\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2228\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;49;00m\n\u001b[32m   2229\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;49;00m\n\u001b[32m   2230\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m_apply_feature_types_on_example\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2231\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_token_per_repo_id\u001b[49m\n\u001b[32m   2232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1677\u001b[39m, in \u001b[36mTakeExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1675\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1676\u001b[39m     ex_iterable_num_taken = \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mnum_taken\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state_dict \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1677\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mex_iterable_num_taken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1678\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1679\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_taken\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1067\u001b[39m, in \u001b[36mMappedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1065\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m key, formatter.format_row(pa_table)\n\u001b[32m   1066\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1067\u001b[39m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1417\u001b[39m, in \u001b[36mFilteredExamplesIterable._iter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1416\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_iter\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmask_column_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1231\u001b[39m, in \u001b[36mMappedExamplesIterable._iter\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1229\u001b[39m             \u001b[38;5;28mself\u001b[39m._state_dict[\u001b[33m\"\u001b[39m\u001b[33mprevious_state_example_idx\u001b[39m\u001b[33m\"\u001b[39m] = current_idx\n\u001b[32m   1230\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformed_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_idx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_state_dict\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1207\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.iter_outputs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1205\u001b[39m         indices.pop(\u001b[32m0\u001b[39m), tasks.pop(\u001b[32m0\u001b[39m)\n\u001b[32m   1206\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1207\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs_iterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapply_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1111\u001b[39m, in \u001b[36mMappedExamplesIterable._iter.<locals>.iter_inputs\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miter_inputs\u001b[39m():\n\u001b[32m-> \u001b[39m\u001b[32m1111\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# If not batched, we can apply the transform and yield the example directly\u001b[39;49;00m\n\u001b[32m   1113\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# first copy the example, since we might drop some keys\u001b[39;49;00m\n\u001b[32m   1114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mformat_dict\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1812\u001b[39m, in \u001b[36mFormattedExamplesIterable.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ex_iterable.iter_arrow:\n\u001b[32m   1810\u001b[39m     \u001b[38;5;66;03m# feature casting (inc column addition) handled within self._iter_arrow()\u001b[39;00m\n\u001b[32m   1811\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, pa_table \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter_arrow():\n\u001b[32m-> \u001b[39m\u001b[32m1812\u001b[39m         batch = \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1813\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m _batch_to_examples(batch):\n\u001b[32m   1814\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m key, example\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\formatting\\formatting.py:471\u001b[39m, in \u001b[36mPythonFormatter.format_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lazy:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyBatch(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m batch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    472\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.python_features_decoder.decode_batch(batch)\n\u001b[32m    473\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\formatting\\formatting.py:151\u001b[39m, in \u001b[36mPythonArrowExtractor.extract_batch\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mextract_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa_table\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\pyarrow\\table.pxi:2311\u001b[39m, in \u001b[36mpyarrow.lib._Tabular.to_pydict\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\pyarrow\\table.pxi:1384\u001b[39m, in \u001b[36mpyarrow.lib.ChunkedArray.to_pylist\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\pyarrow\\array.pxi:1812\u001b[39m, in \u001b[36mpyarrow.lib.Array.to_pylist\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\pyarrow\\scalar.pxi:990\u001b[39m, in \u001b[36mpyarrow.lib.ListScalar.as_py\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\pyarrow\\array.pxi:1812\u001b[39m, in \u001b[36mpyarrow.lib.Array.to_pylist\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\pyarrow\\scalar.pxi:1085\u001b[39m, in \u001b[36mpyarrow.lib.StructScalar.as_py\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:825\u001b[39m, in \u001b[36mkeys\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen _collections_abc>:850\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, mapping)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting data load and targeted filtering...\")\n",
    "\n",
    "data_stream = load_dataset(\n",
    "    \"openfoodfacts/product-database\",\n",
    "    split=\"food\",\n",
    "    streaming=True # Use streaming for massive datasets\n",
    ")\n",
    "\n",
    "filtered_data_stream = data_stream.filter(filter_target_products)\n",
    "df_initial = pd.DataFrame(filtered_data_stream.take(5000))\n",
    "\n",
    "df_keywords_filtered = filter_df_by_keywords(df_initial, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca6bb0-7b83-465c-b096-93e1145625bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
