{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265cef27-36dd-4457-8943-276d5845aba3",
   "metadata": {},
   "source": [
    "# Chocolate Database analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ae4ae-a2fd-4fd1-bfe9-c7fa6487c1bf",
   "metadata": {},
   "source": [
    "The Open Food Facts is a collaborative project where individual can contribute by adding data from the food products they buy.\n",
    "This project is a practice for visualization and classification specificaly on chocolate products. Here is the list of ideas, questions I wanted to investigate:\n",
    "1. As any chocolate lover knows, there are several types of chocolate bar based on the amount of actual cocoa in the product. My hypothesis is that those categories should have a direct influence on the nutrition values (fat, carbohydrates, protein, etc.). Hence the first idea is to create a model to classify the different types of chocolate bars.\n",
    "2. Based on the identified types, see the distribution for the nutrients and the prevalence of certain categories by brands.\n",
    "3. Map the countries of origin with a visual for the number of product of that origin.\n",
    "4. Map the dominant type of chocolate for each consumer coutry.\n",
    "\n",
    "With no further ado, let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8df8c2-d238-4bc9-9b4e-0e6f0daf646c",
   "metadata": {},
   "source": [
    "## Data Loading and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c4eb364-e103-4a2b-a5af-c7645140e4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Value, Features\n",
    "import pandas as pd #version 2.3.3\n",
    "import numpy as np #version 2.3.1\n",
    "import seaborn as sns #version 0.13.2\n",
    "import matplotlib #version 3.10.0\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm #version 0.14.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea138b2-2fd1-4bd2-8869-728681b0ffc6",
   "metadata": {},
   "source": [
    "To begin with, a small sample of the database is loaded to evaluate the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4e1c74e-86aa-4bfe-82b7-9ff119b7b7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_stream = load_dataset(\n",
    "    \"openfoodfacts/product-database\",\n",
    "    split=\"food\",\n",
    "    streaming=True # Use streaming for massive datasets\n",
    "    )\n",
    "df_initial = pd.DataFrame(data_stream.take(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ec1c5-908c-4adb-b6b9-d5bc0fd78c4a",
   "metadata": {},
   "source": [
    "The database contains many variables that are not relevant for our goal. With 110 columns, there are only a handful that would be of interrest. So that would be the first place to start cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "28f5e04f-2a35-49bb-b98b-b893eafaab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_by_keywords(df, keywords):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to include only columns whose names contain any of the given keywords.\n",
    "    \"\"\"\n",
    "    # Create a list of columns where the column name (lowercased) contains any of the keywords\n",
    "    relevant_cols = []\n",
    "    for col in df.columns: \n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in col.lower():\n",
    "                relevant_cols.append(col)\n",
    "    return relevant_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a737430-fa14-4546-b8ee-323ca9e15a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_keywords = [\"name\", \"Quantity\", \"Brands\", \"Categories\", \n",
    "                    \"Manufacturing\", \"Stores\", \"Country\", \n",
    "                     \"Ingredients\",\"Origin\", \"nutriments\"]\n",
    "\n",
    "target_col = filter_df_by_keywords(df_initial, targeted_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052dcea2-ee1a-4f5a-b206-4d411a906b1e",
   "metadata": {},
   "source": [
    "This first helped to narrow down to 34 columns. Unfortunately, even with filtering with keywords, there are several columns that are not useful. At this stage, a manual verification is the best option for selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a0ec8a12-46fd-490d-a6d4-64a38941d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "items_to_remove = [\"categories\", \"categories_properties\",\"ingredients_analysis_tags\", \n",
    "                   \"ingredients_from_palm_oil_n\", \"quantity\", \"ingredients_text\",\n",
    "                  \"ingredients_with_specified_percent_n\", \"ingredients_with_unspecified_percent_n\",\n",
    "                  \"ingredients_without_ciqual_codes_n\", \"ingredients_without_ciqual_codes\",\n",
    "                 \"known_ingredients_n\", \"ingredients_n\", \"unknown_ingredients_n\"]\n",
    "\n",
    "removal_set = set(items_to_remove)\n",
    "target_col_clean = [\n",
    "        item for item in target_col\n",
    "        if item not in removal_set\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80db7f1-6eb2-4b16-af33-72fc1d203c80",
   "metadata": {},
   "source": [
    "Now we are down to 21 columns. there is still some cleaning work to unpack certain columns that contains list of dictionnaries and string with specific format. But it is more interesting to put that in please with the definitive data. Hence, it is necessary to load only the data related to chocolate. As tested on the advanced research option of the Open Food Facts website, setting Category to Chocolate and requesting Cocoa to be part of the ingredient is a good method to get chocolate product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3f97dcba-cd4d-4734-b222-f799b629f9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_substring(categories_list: list, search_term: str) -> bool:\n",
    "    \"\"\"\n",
    "    Helper function to check if the target substring is present in the list.\n",
    "    \"\"\"\n",
    "    # Handle NaN/None values safely: if the cell is empty, treat it as an empty list\n",
    "    if categories_list is None or not isinstance(categories_list, list):\n",
    "        return False\n",
    "        \n",
    "    # Use any() to check if AT LEAST ONE item in the list contains the search term\n",
    "    return any(search_term in item.lower() for item in categories_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b746bdf0-7d4b-4483-ba89-0cf6994b817c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'Value' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# creating a dictionary object to pass to the load_dataset to load only the targeted columns\u001b[39;00m\n\u001b[32m      2\u001b[39m selected_features = {item:Value(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m target_col_clean}\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m data_stream = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mopenfoodfacts/product-database\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfood\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mselected_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstreaming\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Use streaming for massive datasets\u001b[39;49;00m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m df = pd.DataFrame(data_stream.take(\u001b[32m1000\u001b[39m))\n\u001b[32m     12\u001b[39m mask_chocolate = df[\u001b[33m'\u001b[39m\u001b[33mcategories_tags\u001b[39m\u001b[33m'\u001b[39m].apply(check_for_substring, args=(\u001b[33m\"\u001b[39m\u001b[33mchocolate\u001b[39m\u001b[33m\"\u001b[39m,))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\load.py:2148\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[39m\n\u001b[32m   2146\u001b[39m \u001b[38;5;66;03m# Return iterable dataset in case of streaming\u001b[39;00m\n\u001b[32m   2147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m streaming:\n\u001b[32m-> \u001b[39m\u001b[32m2148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_streaming_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2150\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m   2151\u001b[39m builder_instance.download_and_prepare(\n\u001b[32m   2152\u001b[39m     download_config=download_config,\n\u001b[32m   2153\u001b[39m     download_mode=download_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2156\u001b[39m     storage_options=storage_options,\n\u001b[32m   2157\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\builder.py:1274\u001b[39m, in \u001b[36mDatasetBuilder.as_streaming_dataset\u001b[39m\u001b[34m(self, split, base_path)\u001b[39m\n\u001b[32m   1271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBad split: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Available splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(splits_generators)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1273\u001b[39m \u001b[38;5;66;03m# Create a dataset for each of the given splits\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1274\u001b[39m datasets = \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1275\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_as_streaming_dataset_single\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1276\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplits_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1278\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(datasets, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1280\u001b[39m     datasets = IterableDatasetDict(datasets)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\utils\\py_utils.py:484\u001b[39m, in \u001b[36mmap_nested\u001b[39m\u001b[34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    483\u001b[39m     data_struct = [data_struct]\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m mapped = \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_struct\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[32m    486\u001b[39m     mapped = mapped[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\builder.py:1290\u001b[39m, in \u001b[36mDatasetBuilder._as_streaming_dataset_single\u001b[39m\u001b[34m(self, splits_generator)\u001b[39m\n\u001b[32m   1288\u001b[39m \u001b[38;5;66;03m# add auth to be able to access and decode audio/image files from private repositories.\u001b[39;00m\n\u001b[32m   1289\u001b[39m token_per_repo_id = {\u001b[38;5;28mself\u001b[39m.repo_id: \u001b[38;5;28mself\u001b[39m.token} \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.repo_id \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mIterableDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1291\u001b[39m \u001b[43m    \u001b[49m\u001b[43mex_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplits_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\n\u001b[32m   1292\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\iterable_dataset.py:1919\u001b[39m, in \u001b[36mIterableDataset.__init__\u001b[39m\u001b[34m(self, ex_iterable, info, split, formatting, shuffling, distributed, token_per_repo_id)\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m distributed \u001b[38;5;129;01mand\u001b[39;00m distributed.world_size > \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m shuffling \u001b[38;5;129;01mand\u001b[39;00m shuffling._original_seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1914\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1915\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe dataset doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt have a fixed random seed across nodes to shuffle and split the list of dataset shards by node. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1916\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease pass e.g. `seed=42` in `.shuffle()` to make all the nodes use the same seed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1917\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1919\u001b[39m info = \u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m DatasetInfo()\n\u001b[32m   1920\u001b[39m DatasetInfoMixin.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, info=info, split=split)\n\u001b[32m   1922\u001b[39m \u001b[38;5;28mself\u001b[39m._ex_iterable = copy.copy(ex_iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\info.py:297\u001b[39m, in \u001b[36mDatasetInfo.copy\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mDatasetInfo\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__dict__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:20\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, description, citation, homepage, license, features, post_processed, supervised_keys, builder_name, dataset_name, config_name, version, splits, download_checksums, download_size, post_processing_size, dataset_size, size_in_bytes)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\info.py:170\u001b[39m, in \u001b[36mDatasetInfo.__post_init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    168\u001b[39m     \u001b[38;5;66;03m# Convert back to the correct classes when we reload from dict\u001b[39;00m\n\u001b[32m    169\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.features \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.features, Features):\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m         \u001b[38;5;28mself\u001b[39m.features = \u001b[43mFeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.post_processed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.post_processed, PostProcessedInfo):\n\u001b[32m    172\u001b[39m         \u001b[38;5;28mself\u001b[39m.post_processed = PostProcessedInfo.from_dict(\u001b[38;5;28mself\u001b[39m.post_processed)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\features\\features.py:1836\u001b[39m, in \u001b[36mFeatures.from_dict\u001b[39m\u001b[34m(cls, dic)\u001b[39m\n\u001b[32m   1810\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1811\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_dict\u001b[39m(\u001b[38;5;28mcls\u001b[39m, dic) -> \u001b[33m\"\u001b[39m\u001b[33mFeatures\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1812\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1813\u001b[39m \u001b[33;03m    Construct [`Features`] from dict.\u001b[39;00m\n\u001b[32m   1814\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1834\u001b[39m \u001b[33;03m        {'_type': Value(dtype='string', id=None)}\u001b[39;00m\n\u001b[32m   1835\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1836\u001b[39m     obj = \u001b[43mgenerate_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(**obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\features\\features.py:1460\u001b[39m, in \u001b[36mgenerate_from_dict\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   1458\u001b[39m \u001b[38;5;66;03m# Otherwise we have a dict or a dataclass\u001b[39;00m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_type\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m obj \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj[\u001b[33m\"\u001b[39m\u001b[33m_type\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: \u001b[43mgenerate_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m obj.items()}\n\u001b[32m   1461\u001b[39m obj = \u001b[38;5;28mdict\u001b[39m(obj)\n\u001b[32m   1462\u001b[39m _type = obj.pop(\u001b[33m\"\u001b[39m\u001b[33m_type\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\conda_envs\\bite_to_dust\\Lib\\site-packages\\datasets\\features\\features.py:1459\u001b[39m, in \u001b[36mgenerate_from_dict\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m   1457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [generate_from_dict(value) \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[32m   1458\u001b[39m \u001b[38;5;66;03m# Otherwise we have a dict or a dataclass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33;43m\"\u001b[39;49m\u001b[33;43m_type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj[\u001b[33m\"\u001b[39m\u001b[33m_type\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m   1460\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {key: generate_from_dict(value) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m obj.items()}\n\u001b[32m   1461\u001b[39m obj = \u001b[38;5;28mdict\u001b[39m(obj)\n",
      "\u001b[31mTypeError\u001b[39m: argument of type 'Value' is not iterable"
     ]
    }
   ],
   "source": [
    "# creating a dictionary object to pass to the load_dataset to load only the targeted columns\n",
    "selected_features = {item:Value(\"string\") for item in target_col_clean}\n",
    "\n",
    "data_stream = load_dataset(\n",
    "    \"openfoodfacts/product-database\",\n",
    "    split=\"food\",\n",
    "    features=selected_features,\n",
    "    streaming=True # Use streaming for massive datasets\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(data_stream.take(1000))\n",
    "mask_chocolate = df['categories_tags'].apply(check_for_substring, args=(\"chocolate\",))\n",
    "mask_cocoa = df['ingredients_tags'].apply(check_for_substring, args=(\"cocoa\",))\n",
    "\n",
    "filtered_df = df[mask_chocolate & mask_cocoa].reset_index(drop=True)\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbbab5-083c-4645-bbb7-7db609702919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_target_products(example):\n",
    "    \"\"\"\n",
    "    Filters records to find products classified as 'chocolate' AND\n",
    "    containing 'cocoa' in the ingredients list, safely handling lists, strings,\n",
    "    and lists of dictionaries for both fields.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Process Categories (Must contain 'chocolate') ---\n",
    "    # Try 'categories_tags', fall back to 'categories' if needed\n",
    "    categories_data = example.get('categories_tags', example.get('categories', ''))\n",
    "    \n",
    "    searchable_categories = ''\n",
    "    if isinstance(categories_data, list):\n",
    "        # Check if the list contains dictionaries \n",
    "        if categories_data and isinstance(categories_data[0], dict):\n",
    "            # Extract the 'name' or 'id' from each dictionary item\n",
    "            string_parts = [item.get('name', item.get('id', '')) for item in categories_data if isinstance(item, dict)]\n",
    "            searchable_categories = ' '.join(string_parts).lower()\n",
    "        else:\n",
    "            # Assume it's a list of strings and join them\n",
    "            searchable_categories = ' '.join(categories_data).lower()\n",
    "    else:\n",
    "        # It's a string, NaN, or other single value\n",
    "        searchable_categories = str(categories_data).lower()\n",
    "        \n",
    "    has_chocolate_category = 'chocolate' in searchable_categories\n",
    "    \n",
    "    \n",
    "    # --- 2. Process Ingredients (Must contain 'cocoa') ---\n",
    "    # Prioritize 'ingredients_text' (the clean string), fall back to 'ingredients'\n",
    "    ingredients_data = example.get('ingredients_text', example.get('ingredients', ''))\n",
    "    \n",
    "    searchable_ingredients = ''\n",
    "    if isinstance(ingredients_data, list):\n",
    "        # Check if the list contains dictionaries (the cause of the TypeError)\n",
    "        if ingredients_data and isinstance(ingredients_data[0], dict):\n",
    "            # Critical fix: Extract the 'text' key from the dictionary objects\n",
    "            string_parts = [item.get('text', '') for item in ingredients_data if isinstance(item, dict)]\n",
    "            searchable_ingredients = ' '.join(string_parts).lower()\n",
    "        else:\n",
    "            # Assume it's a list of strings\n",
    "            searchable_ingredients = ' '.join(ingredients_data).lower()\n",
    "    else:\n",
    "        # It's a string (like the pre-joined ingredients_text), NaN, or other single value\n",
    "        searchable_ingredients = str(ingredients_data).lower()\n",
    "    \n",
    "    has_cocoa_ingredient = 'cocoa' in searchable_ingredients\n",
    "    \n",
    "    return has_chocolate_category and has_cocoa_ingredient\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6740baa6-de5c-4b19-817d-7dbae78865a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting data load and targeted filtering...\")\n",
    "\n",
    "data_stream = load_dataset(\n",
    "    \"openfoodfacts/product-database\",\n",
    "    split=\"food\",\n",
    "    streaming=True # Use streaming for massive datasets\n",
    ")\n",
    "\n",
    "filtered_data_stream = data_stream.filter(filter_target_products)\n",
    "df_initial = pd.DataFrame(filtered_data_stream.take(5000))\n",
    "\n",
    "df_keywords_filtered = filter_df_by_keywords(df_initial, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca6bb0-7b83-465c-b096-93e1145625bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
